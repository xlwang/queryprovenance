
\noindent
\textbf{Comment \#3:} Scalability 
\begin{quote}
\reviewer{
\dots relatively small datasets are tested
\dots I suggest to explicitly test the scalability until the bottlenecks of the solutions become clear (the MILP module?)
}
\end{quote}

For the revision, we have extended our experiments and augmented
Figure~\ref{f:attr100} to datasets of up to $100K$ records. This experiment
uses the most complex setting (\texttt{UPDATE} queries with \textit{range}
\texttt{WHERE} clause), and shows that \sys is efficient for datasets of
$100K$ records, even when the query corruption is not recent.

In fact, there exists two scalability bottlenecks: the bottleneck of \sys in generating MILP problem that is solvable in 
a reasonable time; and the bottleneck of hardware we use to operate \sys. 
These two bottlenecks are closely interact with each other as better hardware solves harder MILP problem faster. 
Problems with $100$ attributes, $100k$ records, and $250$ queries already 
reach the memory limitation of machines we used, thus we did not further extend \sys to even larger database sizes. 



\comskip

\noindent
\textbf{Comment \#4:} Accuracy metrics
\begin{quote}
\reviewer{
I cannot grasp from the experiments section if the authors are counting the
number of errors correctly identified or correctly fixed.
\dots
this implies that even complete complain sets do NOT guarantee that the query
is correctly fixed and future errors will not occur \dots
}
\end{quote}

We apologize for the confusion and we have explained our accuracy metrics in Section~\ref{sec:setup}.

The accuracy metrics in Section~\ref{sec:experiments} measure whether the complaint tuples 
are repaired correctly, but it is possible that the repaired query differs from the true query. 
In the example, both $income \geq 87200$ and $income \geq 87500$ 
resolves all the complaints and thus have the same accuracy. 
We separately evaluate whether \sys selects the right query to repair; 
these results are included in Appendix~\ref{app:index}.  
In summary, we find that \sys always fixes the right query when the complaint 
set is complete.  However, the less complete the complaint set, and the older 
the corruption, the more likely it is that \sys will repair the wrong query.
At current stage, \sys does not guarantee to derive query repairs that are 
exactly same as the true query even in problems with complete complaint sets. 

In the example, we also consider $income >= 87200$ as a
correct query repair as it resolves all the reported complaints. In addition,
the repaired query is allowed to differ from the true query. However, this is
very rare in practice and we show that \sys always pick the correct query to
fix when the complaint set is complete. Though \sys dos not guarantee to
propose a query log repair that is exactly the same with the original true
query log, it identifies the incorrect query and provides a reasonable
suggestion of how to fix the problem.

These metrics measure whether the complaint tuples are repaired correctly, but it is possible that the repaired query differs from the true query. 
We separately evaluate whether \sys selects the right query to repair; these results are included in Appendix~\ref{app:index}.  In summary, we find that \sys always fixes the right query when the complaint set is complete.  However, the less complete the complaint set, and the older the corruption, the more likely it is that \sys will repair the wrong query.

\comskip

\noindent
\textbf{Comment \#5:} Error generation
\begin{quote}
\reviewer{
I believe the way that errors are introduced can be improved.
\dots Randomly introducing errors do not guarantee that complex patterns (such as the one in Figure 1) happen 
\dots
Unfortunately having some guarantee on the way that errors are introduced can
be a challenging problem \dots
}
\end{quote}

In this paper, we introduce a synthetic data generator in Section~\ref{sec:setup}
with multiple adjustable parameters, including query type, workload and
database size, and query selectivity.  These parameters allow us to generate 
synthetic workloads on \texttt{UPDATE}, \texttt{INSERT}, and \texttt{DELETE} queries and
further helps manipulating level of query interaction for \texttt{UDPATE}-workloads. 
We found that \texttt{UPDATE} queries has higher level of query interaction and thus
requires longer time to solve than \texttt{INSERT} and \texttt{DELETE} queries 
(Figure~\ref{f:indelup_time}). Under our default setting, \texttt{UPDATE} queries with 
\textit{range} predicates and \textit{relative} \texttt{SET} clauses have higher level of query
 interaction than \textit{point} predicates and \textit{constant} 
\texttt{SET} clauses respectively, thus problems with the former settings are 
harder to solve than the latter settings (Figure~\ref{f:qidx_time}). 
In addition, we also study the \sys's performance over two parameters that 
contributes to the query interaction probability: the number of attributes, $N_a$, in 
Figure~\ref{f:attr} and percentage of query selectivity in Appendix~\ref{app:selectivity}.
Our experiment confirms that higher level of query interaction leads to 
harder problems (requires longer time to solve). 

\alex{We should talk about this here.}


\comskip

\noindent
\textbf{Comment \#6:} Connection to data repair
\begin{quote}
\reviewer{
the MILP formulation reminds me the SAT formulation in [25] and similar works
for data cleaning. In fact def 4 is very close to the minimal repair concept
in data repair. This can be clarified.
}
\end{quote}

We thank the reviewer for mentioning these data repairs works. In fact, 
\sys adopt similar intuition in these data repair works, 
which seek for minimum repair on data, 
to the query repair problem we studied in this paper. 

\alex{Have we done something for this?} \xlw{let's cite these papers when explaining def 4. }

%!TEX root = ../main.tex

%
% Notes: 
%
%  exact implementation
%  comparison with rollback at batch N
%  relaxing the solvers
%  dealing with false positives
%
%  running on TPC-C/sanjay's.  equality is easier
%
% Need names for
%  * rollback
%  * exact solution
%  * fixing an individual/batch of queries
%
\section{Experiments}

\ewu{Should say this somewhere: 
Updates are often a small fractice of a query workload -- typically $10\%$~\cite{} in most benchmarks --
thus in practice \sys can work for much larger workloads.  In the experiments, we filtered the workloads to 
only look at modification queries, and specifically UPDATE queries which are hard.
}


In this section, we carefully study the performance and accuracy
characteristics of the naive MILP-based repair algorithm, as well as 
slicing-based optimizations that improve the latency of the system over the naive
approach by over $Y\times$, an incremental algorithm for single query corruptions that
further improves the latency over the naive approach by $>Y\times$, and 
the multi-pass tuple-slicing algorithm that tolerates incomplete complaint sets with minimal loss in accuracy.
Our goals in the evaluation is to understand these trade-offs in
controlled synthetic scenarios, as well as study the effectiveness
in typical database query workloads based on widely used benchmarks.

To this end, our experiments are organized as follows: First, 
we compare the exhaustive MILP algorithm against the different optimizations 
to highlight the value of different optimizations.  We then compare the
repair costs of \texttt{INSERT}, \texttt{DELETE}, or \texttt{UPDATE}-only query logs 
and find that the latter query type is by far the most complicated and costly to repair.
For the subsequent experiments, we focus solely on \texttt{UPDATE}-only synthetic workloads 
to understand how \sys responds to different query logs and databases.  
Finally, we evaluate the efficacy on real-world databases transaction benchmarks,
TPC-C XXX~\cite{tpcc} and AuctionMark~\cite{auctionmark}.
All experiments were run on \ewu{X Core} 2.66 GHz  machines with 16GB RAM running \ewu{OS + Version}.



% establish the quality limitations of existing heuristics and the need for a formal, 
% constraint-based algorithm (\exact).  Second, we study how each of the 
% optimizations described in Section~\ref{s:optimiztaions} improves algorithm scalability.
% Third, we introduce different forms of error in the input complaint sets and study the 
% effectiveness of our noise-handling heuristics.  




%
% NOTE: figures are named <experimentsection>_<subsection>_<xaxis>.pdf
%

\subsection{Experimental Setup}


\begin{table}[t]\small
  \centering
  \begin{tabular}{@{}cll@{}}
  \toprule
  {\bf Param} & {\bf Description} & {\bf Default} \\ \midrule
  $V_d$  & Domain range of the attributes  & $[0, 100]$ \\
  $N_D$  & \# tuples in final database & $1000$ \\
  $N_a$  & \# attributes in database & $10$ \\
  $N_w$  & \# predicates in \texttt{WHERE} clauses & $1$ \\
  $N_s$  & \# \texttt{SET} clauses & $1$ \\
  $N_q$  & \# queries in query log & $300$ \\
  $idx$  & Index of corrupted query & $\{0, 25, 50,$ \\
         & (backwards from most recent) & $100, 200, 250 \}$ \\ %$\frac{N_q}{2}$ \\
  $r$    & Range size of \texttt{UPDATE} queries & 8 (tuples) \\
  $s$    & Zipf $\alpha$ param of query attributes, & $1$ \\ 
  $set$  & Constant vs relative \texttt{SET} clauses. & const \\ \bottomrule \end{tabular}
         %& power low distribution $P(v) = v^{-s}$ & \\\end{tabular}
  \caption{Experimental Parameters}
  \label{t:params}
\end{table}


\iffalse
  \begin{table}[t]\small
    \centering
    \begin{tabular}{@{}cl@{}}
    \toprule
    {\bf Param} & {\bf Description} \\ \midrule
    $p$ & Precision: \% of repaired tuples that are correct. \\
    $r$ & Recall: \% of full complaint set repaired.\\
    % $t_{prep}$ & Time to construct CPLEX problem \\
    % $t_{send}$ & Time to send CPLEX problem to solver \\
    % $t_{solve}$ & Time for solver to generate a solutions\\
    $t_{total}$ & End-to-end execution time \\ 
    $d_{measure}$ & \red{Some sort of distance measure} \\ \bottomrule \end{tabular}
    \caption{Metrics Compared}
    \label{t:metrics}
  \end{table}
\fi




Each of our experiments follows a consistent procedure. 
We generate a sequence of queries using a synthetic query generator or 
the benchmark program, and corrupt the query log as described below. 
We then execute the original and corrupt query logs on an initial (possibly empty) database,
and perform a tuple-wise comparison between the resulting database states 
to generate a true complaint set.  
We then add noise to the complaint set by 1) picking random tuples not in the true
complaint set to add false positive complaints, and 2) removing true complaints to simulate false negatives.
Finally, we execute the evaluated algorithms on the complaints and compare the fixed
query log with the true query log, as well as the fixed and true
final database states to measure performance and accuracy metrics.

We compare the following algorithms:
$\sys_{exh}$ is the naive, exhaustive algorithma 
$\sys_{S \subseteq \{t,q,a,inc\}}$, where the subscripts $t,q,a,inc$ represent the
application of tuple, query and attribute slicing and single query incremental repairs, 
and $S$ defines the set of optimizations present in the algorithm.
For example, $\sys_{t,q,inc}$ uses both tuple and query slicing along with incremental repair.
When the algorithm uses incremental computation, the superscripts $\sys_{inc}^{1st}$
and $\sys_{inc}^{all}$ variants that either returns the first successful repair,
or the repair with the lowest objective function across the entire query log.

We evaluate the algorithms along several metrics.  Performance is measured as wall clock
time between submitting a query log and the system terminating after retrieving all relevant repairs 
(e.g., the time to the first repairt for $\sys_{inc}^{1st}$, or the time to process the full log for $\sys{inc}^{all}$).  
Experiment~\ref{???} evaluates the time to output {\it each repair} when running the 
incremental variant across the entire query log.  

We also measure the repair's precision (percentage of repaired tuples that were correctly fixed), 
the recall (the percentage of the full complaint set that was repaired), 
and the F1 measure (the harmonic mean of precision and recall).
We note that in the context of complete complaint sets, the recall will always be $1$ if \sys returns a result.
The recall can only degrade due to solver infeasibility (the solver does not find a valid assignment within a loose time bound), 
or in the context of incomplete complaint sets.   Thus our reported metrics are the average across multiple runs.
Finally, Table~\ref{t:params} summarizes the key parameters that we vary throughout our experiments.  
We describe them in the context of the datasets and workloads below.



\subsubsection{Datasets and Workloads}

This subsection describes the query and data generation process in greater detail.

\stitle{Synthetic:} \label{sec:syntheticgen}
We generate an initial database of $N_D$ random tuples.  
The schema contains $N_a=5$ attributes $a_1\ldots a_5$, whose values are
picked from $V_d$ uniformly at random, along with a primary key $id$.
We then generate a sequence of $N_q$ \texttt{UPDATE} queries~\footnote{\scriptsize We focus on
\texttt{UPDATE} only query logs because they are the {\it predominant} cost 
in a query log.  Experiment~\ref{???} compares \texttt{INSERT}, \texttt{DELETE} and \texttt{UPDATE}
only workloads to illustrate the differenc \ewu{Experiment could move to appendix}.} where 
\verb|?| is a randomly generated value and \verb|r| is the size of the range predicate: 
{\scriptsize
\begin{verbatim}
  UPDATE SET (a_i = ?),.. WHERE a_j = ? AND ...
  UPDATE SET (a_i = ?),.. WHERE a_j in [?, ?+r] AND ...
\end{verbatim}
}

The $set$ parameter controls whether the \texttt{UPDATE} queries set attributes to random constant values ({\it const}),  
or relative values ({\it rel}) by incrementing by a random, possibly negative, value.  
The \texttt{WHERE} clauses form a conjunction.  
In addition, the skew parameter $s$ determines the distribution attributes referenced in the \texttt{WHERE} and \texttt{SET} clauses.  
Each attribute in a query is picked from either a uniform distribution when $s=0$ or a zipfian~\cite{zipf} distribution.
This allows our experiments to vary between a uniform distribution, where each attribute is
equally likely to be picked, and a skewed distribution where nearly all attributes are the same. \\
The \texttt{WHERE} clauses in \texttt{DELETE} queries are generated in an identical fashion, while
\texttt{INSERT} queries insert values picked uniformly at random from $V_d$.



\stitle{TPC-C:} We use the data and query workload over the {\it ORDER} table in TPC-C~\cite{tpcc}.  
We generated a database at scale 1 with one warehouse, and kept the queries that modify the
{\it ORDER} table. The initial table contains 4570 tuples and we ultimately generated a log with
1000 queries, where $530$ are \texttt{UPDATE}s and the rest are \texttt{INSERT}s.

\stitle{AuctionMark:} Auctionmark workload simulates the 
actual action market. We generate a database 
with 8729 tuples and $1000$ queries, with $540$ \texttt{UPDATE}s and $110$ \texttt{INSERT}s.
Both TPC-C and AuctionMark setups were generated using the OLTP-bench project~\cite{oltpbench,oltpbenchgit}.


\stitle{Corrupting Queries:} We corrupt query $q_i$ by replacing it with a randomly
generated query of the same type based on the procedures described above.
To standardize our procedures, we selected a fixed set of indexes $idx$
that are used in all experiments.  Appendix~\ref{app:qidx} presents
experiments that justify the rationale behind our selection.

  \begin{figure*}[h]
  \centering
    \vspace*{-.2in}
    \begin{subfigure}[t]{.3\textwidth}
    \includegraphics[width = .99\columnwidth]{figures/scale_allalgs}
    \vspace*{-.1in}
    \caption{.}
    \label{f:multiquery} 
    \end{subfigure}
    \begin{subfigure}[t]{.3\textwidth}
    \includegraphics[width = .99\columnwidth]{figures/incrementalcompare_time}
    \vspace*{-.1in}
    \caption{.}
    \label{f:singlequeryinc_time} 
    \end{subfigure}
    \begin{subfigure}[t]{.3\textwidth}
    \includegraphics[width = .99\columnwidth]{figures/placeholder}
    \vspace*{-.1in}
    \caption{.}
    \label{f:qtype} 
    \end{subfigure}
    \\
    \begin{subfigure}[t]{.3\textwidth}
    \includegraphics[width = .99\columnwidth]{figures/scale_allalgs}
    \vspace*{-.1in}
    \caption{.}
    \label{f:multiquery} 
    \end{subfigure}
    \begin{subfigure}[t]{.3\textwidth}
    \includegraphics[width = .99\columnwidth]{figures/incrementalcompare_acc}
    \vspace*{-.1in}
    \caption{.}
    \label{f:singlequeryinc_acc} 
    \end{subfigure}
    \begin{subfigure}[t]{.3\textwidth}
    \includegraphics[width = .99\columnwidth]{figures/placeholder}
    \vspace*{-.1in}
    \caption{.}
    \label{f:qtype} 
    \end{subfigure}
    \vspace*{-.1in}
    \caption{Preliminary Experiments}
  \end{figure*}


\subsection{Preliminaries}
The following set of experiments are designed to establish the rationale for 
our experimental settings in the subsequent experiments.  
Specifically, we compare the exhaustive approach with the suite of optimizations
as the number of corrupted queries increases.  
We find that \ewu{XXX}.
We then establish the difficulty of \texttt{UPDATE} workloads as compared to other query types.



\stitle{Multiple Corrupt Queries:}
In this experiment, we compare the naive exhaustive algorithm with four types of algorithms
$\sys_S$, where $S \in \{t, q, a, tqa\}$.  We use the default settings with $1000$ tuples and
$50$ \texttt{update} queries.  We corrupt every tenth query starting from oldest query $q_0$,
up to $q_{40}$.  For example, when we corrupt $3$ queries, we corrupt $q_{0,10,20}$.


\stitle{Single Query Optimization:}
In this experiment, we evaluate the efficacy of the incremental repair algorithm (Section~\ref{sec:inc})
in the special case when only one query has been corrupted.  We compare the exhaustive 
algorithm against $\sys_{inc}^{1st}$ and $\sys_{inc}^{all}$ using the default parameters.
We also compare against $\sys_{t,inc}^{1st}$ to illustrate the value of the second MILP iteration in tuple slicing.
Figure~\ref{f:singlequeryinc_time} highlights the scalability limitations of the exhaustive algorithm 
even $25$ queries in this experiment.  In contrast, the incremental algorithms perform at nearly the same rate
across the different query indexes.  Figure~\ref{f:singlequeryinc_acc} shows the value of tuple-slicing --
both basic incremental algorithms have severely degraded precision measures due to their propensity to over-generalize
-- however the introduction of the second MILP iteration significantly improves the precision.  
In all algorithms, the recall is near $1$ because the complaint sets are complete and the solver is able to find 
an assignment in nearly all cases.


\stitle{Query Type Experiment:}
Our final preliminary experiment evaluates the exhaustive, tuple-slicing algorithm 
$\sys_{exh,t}$ on insert, delete, or update-only workloads.
We increase the number of queries from $20$ to $100$ and corrupt the oldest query in the log.  
The rest of the parameters use the default values.
Figure~\ref{f:qtype} shows xxx.


{\it Discussion: 
End up picking $queryfix_{inc,t}$, $queryfix_{inc,t,q}$
Justify single corrupt query
}


\subsection{Synthetic Experiments}
In the following set of synthetic experiments, we vary 
the parametetrs skewness $s$, log size $N_q$, database size $N_D$, and
query complexity $N_w$ one at a time in order  to tease apart the parameters that most affect our metrics.
As described above, we focus query logs with a single corruption,
and on a small set of representative algorithms \ewu{XXX}.


  \begin{figure*}[h]
    \centering
    \begin{subfigure}[t]{.3\textwidth}
      \includegraphics[width = \columnwidth]{figures/placeholder}
      \vspace*{-.1in}
      \caption{.}
      \label{f:scale1} 
    \end{subfigure}
    \begin{subfigure}[t]{.3\textwidth}
      \includegraphics[width = \columnwidth]{figures/dbsize_time}
      \vspace*{-.1in}
      \caption{.}
      \label{f:scale2} 
    \end{subfigure}
    \begin{subfigure}[t]{.3\textwidth}
      \includegraphics[width = \columnwidth]{figures/where_time}
      \vspace*{-.1in}
      \caption{.}
      \label{f:scale3} 
    \end{subfigure}
    \\
    \begin{subfigure}[t]{.3\textwidth}
      \includegraphics[width = .95\columnwidth]{figures/placeholder}
      \vspace*{-.1in}
      \caption{.}
      \label{f:scale1} 
    \end{subfigure}
    \begin{subfigure}[t]{.3\textwidth}
      \includegraphics[width = .95\columnwidth]{figures/dbsize_acc}
      \vspace*{-.1in}
      \caption{.}
      \label{f:scale2} 
    \end{subfigure}
    \begin{subfigure}[t]{.3\textwidth}
      \includegraphics[width = .95\columnwidth]{figures/where_acc}
      \vspace*{-.1in}
      \caption{.}
      \label{f:scale3} 
    \end{subfigure}
    \caption{cap}
  \end{figure*}


\stitle{Skew:}
We generate query log at 4 different skewness levels from uniform to 
super skewed with $s = 0, 1e-7, 0.5, 1$. As we can see as query log
is more skewed, the execution time for solving same amount of queries 
increase. This is also due to the fact that increasing skewness also
increase the query dependency, which, in turn, increase the searching
space in the MILP problem.
 
  \begin{figure}[h]
    \centering
    \begin{subfigure}[t]{.9\columnwidth}
    \includegraphics[width = \columnwidth]{figures/skew_time}
    \caption{.}
    \label{f:multiquery} 
    \end{subfigure}
    \begin{subfigure}[t]{.9\columnwidth}
    \includegraphics[width = \columnwidth]{figures/skew_acc}
    \caption{.}
    \label{f:multiquery} 
    \end{subfigure}
    \caption{Log size. }
  \end{figure}




\stitle{Scalability - Log Size:}

Figure~\ref{f:scale1} shows xxx.

\stitle{Scalability - DB Size:}
We adjust the initial database sizes from 100 to 100,000 and 
while maintain the same range $r$ in each query. As we can see, 
for the same corrupt query index, the MILP-based approaches require
similar amount of time to derive the log repair. 

Figure~\ref{f:scale2} shows xxx.



\stitle{Scalability - Query Complexity:}
In the end, we evaluate the MILP-based approaches by adjust 
the query complexity: number of predicates, $N_w$, in the 
update query while maintain the range $r$ the same. 

Figure~\ref{f:scale3} shows xxx.


\stitle{Incomplete Complaint Sets:}
In this set of experiments, we add false negative complaints to the input
complaint set and evaluate our noise handling algorithms (Section~\ref{sec:noise}).
To generate false negatives ($FN$), we select a random subset of the true complaint set.


  \begin{figure}[h]
    \centering
    \begin{subfigure}[t]{.48\columnwidth}
    \includegraphics[width = .95\columnwidth]{figures/placeholder}
    \caption{.}
    \label{f:falsenegative} 
    \end{subfigure}
    \caption{cap}
  \end{figure}

Figure~\ref{f:falsenegative} varies $FN \in \{\}$.

\stitle{Incremental Results}
INcremental algorithm, incremental results






%
% compair everything, naive sucks
% 
% add compare everything with multiple corrupted queries
% - exh, tupleslice, queryslicing, attrslicing, tuplequeryslicing
% - 1 error: oldest query
% - 2 errors: oldest query + 10th
% - 3 errors: oldest + 10th + 20th
% - dbsize: 1000
% - uniform skew
% 
% zoom in onto: inc, inc+tupleslicing, inc+query slicing, inc+attrslicing, everything (cplex2)
% 
% pick cplex2 as the winner
% 
% cplex_searchall = incremental but for everything
% 
% cplex3 == cplex_stop_early_2iter
% 
% the naive algorithms suck.
% 
% incremental comptuation vs non incremental comptutaion
% 
% show that w/wout query fix is what makes the difference
% incremental should always be on




\subsection{Benchmark}
We evaluate the proposed algorithm on two benchmarks. Queries in the benchmark 
workloads are PRIMARY KEY updates, which modifies one tuple at a time, and insert queries. 
This property greatly simplifies the problem and reduces searching space since queries dependency 
become very sparse: we say one query depends on another only if the result of the latter
query may affects the result of the first one. Thus, in a workload with PRIMARY KEY
updates, two queries have this dependency relationship only
when they have the exactly same where condition, which has much lower probability than 
general updates. As a result, we observe that the proposed approaches work
very efficient and effective 
in fixing errors in both TPC-C and AuctionMark benchmarks. 

\stitle{TPC-C: }

\stitle{AuctionMark: }


\begin{figure}[h]
\centering
  \begin{subfigure}[t]{.48\columnwidth}
  \includegraphics[width = .95\columnwidth]{figures/placeholder}
  \caption{.}
  \label{f:tpcc} 
  \end{subfigure}
  \begin{subfigure}[t]{.48\columnwidth}
  \includegraphics[width = .95\columnwidth]{figures/placeholder}
  \caption{.}
  \label{f:auctionmark} 
  \end{subfigure}
  \caption{Benchmark Performance}

\end{figure}















\iffalse
\subsubsection{Algorithms}

\sys fixes query logs in two distinct steps: first, we filter the query log using 
provenance information and roll back the queries to compute the $ideal$ states of the database.
Second, we apply the solver algorithms (Section~\ref{}) to speculatively fix the query.

We consider the roll-back algorithm in batches of $n_{rollback}$.

\begin{itemize}
\item Combined:  combine rollback and query fixing in a single CPLEX problem
\item $R_n-CPLEX$: 
\item DT
\item Box,Density
\end{itemize}



\subsubsection{Comparison}

\begin{itemize}
\item Query By Example algorithm
\item Quoc's ConQueR
\end{itemize}

Conditions, given a database ${\cal D}$ and query log $qlog$:

\begin{itemize}
\item {\bf $N_\mathcal{D}$: } Size of the database (number of tuples)
\item {\bf $N_{dim}$:} Dimensionality of the database.
\item {\bf $N_\mathcal{Q}$:} Vary number of queries in $qlog$.
\item {\bf $N_{pred}$:} The number of predicates in each UPDATE query's WHERE condition.
\item {\bf $N_{ins}$: } When corrupting the log, the number of values in INSERT to corrupt.
\item {\bf $N_{set}$: } When corrupting the log, the number of clauses in SET that are corrupted.
\item {\bf $N_{where}$: } When corrupting the log, the number of attributes in the WHERE clause that are corrupted.
\item {\bf $idx \in [0, 1]$: } The index of the query in the query log that was corrupted as a percentage of the query log.  
      For example $Idx = 0.0$ is the oldest query in the log, whereas $Idx = 1.0$ is the most recent query position.
\item {\bf $p_{I}$: } Percentage of INSERT queries in the query log (as compared to UPDATEs).
\item {\bf $p_{pk}$: } Percentage of UPDATE queries with primary key filter clauses as compared to range clauses over non-primary key attributes.
\item {\bf $p_{FP}$: } Percentage of false positives in the complaint set.
\item {\bf $p_{FN}$: } Percentage of false negatives in the complaint set.
\end{itemize}



\subsection{Exact Experiments}

\begin{itemize}
\item $N_\mathcal{D} \in \{10, 100, 1000\}$
\item $N_q \in \{10, 20, 50, 100\}$
\item $N_{dim} = 4$
\item $N_{pred} \in \{1, 2, 3\}$
\item $N_{where} = \{1, 2\}$
\item $Idx = \{0, 0.5, 1\}$
\end{itemize}

\subsection{Rollback and \qfix Microexperiments}


The first set of experiments seeks to understand the effectiveness of the database rollback
algorithm.  We use a synthetic dataset (\ewu{describe}) and execute the rollback algorithm
while varying the query batch size.

\subsection{End-to-end experiments}

\subsubsection{Complete Complaint Set}

\subsubsection{Complaint Set with Noise}

\begin{itemize}
\item $N_\mathcal{D} \in \{10, 100, 1000\}$
\item $N_q \in \{10, 20, 50, 100\}$
\item $N_{dim} = 4$
\item $N_{pred} \in \{1, 2, 3\}$
\item $N_{attrs} = \{1, 2\}$
\item $Idx = \{0, 0.5, 1\}$
\end{itemize}

\subsection{TPC-C Experiment}


\deprecate{
  \subsection{Single-Query Log}

  In the first set of experiments, we evaluate the simplest case where there
  is a single update query.  In each experiment, we vary the DBSize,
  NClauses, as well as the number of clauses in the query that have
  been corrupted and report the metrics described above.  We first 
  compare the learning algorithms on a complete complaint set, then evaluate them
  using incomplete complaint sets with varying percentages of false positive and negative complaints.

  \subsubsection{Complete Complaints}

  {\it Vary DBSize

  Vary NClauses, corrupt 1 and 2 clauses
  }

  We found that CPLEX and BBOX identify the correct fix, however their
  running times are significantly higher than DTree.  This is because
  CPLEX is an exact solution, as compared to DTree, whose poor early
  splitting decisions can adversely affect the final tree structure.

  \subsubsection{Incomplete Complaints}


  {\it 
  Vary DBSize

  Vary NClauses, corrupt 1 and 2 clauses

  Each line is plots has different perc FP
  }

  We first increased the number of false positives in the complaint set (no false negatives).
  Figure~\ref{f:single_incomplete_fp} shows how the fix quality and running time vary as the
  percentage of false positives increases.   Compared variations of CPlex and Bounding box with varying
  density thresholds (?).

  Each line varies perc FN

  We then varied the number of false negatives while keeping the percentage of false positives fixed at 5\%.
  Figure~\ref{f:single_incomplete_fn} 


  \subsection{Increased Query Log Size}

  In the following set of experiments, we increase the number of
  queries in the log while varying ?.  The number of corrupted queries
  is still one.  In these experiments, we set the DBSize to 10000,
  the default NClauses to 4, and the number of corrupted clauses to
  2.   We first show results for varying the false positives and
  negatives in the complaint set and comparing the algorithms described
  in Section~\ref{s:incomplete-algs}.  We then evaluate the efficacy
  of of provenance-based query log filtering, which reduces the running
  time without affecting the result quality.

  To generate the false-positives, we randomly sample without replacement
  from the tuples in the database that are not in the true complaint
  set.

  \subsection{False Positive}

  Vary false positives (1 graph)


  \subsection{False Negative}

  Vary false negatives (1 graph)

  \subsubsection{Filtering Queries}

  We also compared the provenance-based filtering techniques in the above experiments
  to measure their effectiveness at reducing the running time.  We varied the complexity of the update 
  WHERE clauses to control the amount that queries in the log overlap in their updates.  The query log contained 50 update WHERE queries.
  The quality of the suggested fixes were the same,  As the clauses became less complex, the likelihood 
  of overlap increased, and increased the amount of queries that affected the complaint sets.


  \begin{figure}[h]
  \centering
  \includegraphics[width = 2in]{figures/complete_qfilter_complexity}
  \caption{Varying query complexity.}
  \label{f:complete_qfilter_complexity} 
  \end{figure}



  \subsection{Multi-Query}

  Using a previous experimental configuration, we varied the number of queries that are corrupted.  Figures~\ref{}
  show the quality and running times of the results for a query log of size 1000 and dbsize of 100k.  
  As we can see, the cost increases quadratically with each corrupted query and the accuracy of the proposed fixes increases marginally.  
  This is because XXX.

  We focus on two scenarios.  {\bf Try multiple corruptions that don't overlap (provenance-wise) with each other}.  This is the condition of multiple silo'd that
  corrupt their own logs.  Then {\bf Try multiple corrupted queries where one query modifies the updated state of the other}.  This shows that
  it is really hard and hopefully we get close?



  To better understand the algorithm, we plot the quality metrics after each fixed query and measure how quickly \sys converges to the final result. 
  This suggests that an incremental approach where the user can set a threshold to stop the algorithm may be effective.



  \subsection{Real Transactional Workload}

  We used the web application workload, and evaluated our alogirthms with artifically injected corruptions.
  We compared two types of corruptinos.  In figure~\ref{f:real_existing}, we randomly picked a single existing 
  query and corrupted its value.  If the query was an INSERT, we randomly pick a value and perturbed it.
  If the transaction was an UPDAET, we randomly varied the SET or WHERE clauses.   We re-ran this
  100 times and plot the average and standard deviation of the results.

}
\fi

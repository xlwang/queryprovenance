
\section{Introduction}
\label{s:intro}


Despite big data, sensors, and automation a huge amount of the most valuable data continues to be generated by humans 
        ( Human and financial resource management systems used in nearly all companies and universities,
         Accounting software,
         DMV,
         Ebay, Paypal, 
         Forums,
         CRUD.  Django, Rails)
In all of these applications, human inputs submitted through forms are translated into a sequence of INSERT, DELETE and UPDATE queries that modify the database.
In essence, these are fundamentally OLTP applications that translate user inputs into stored procedure parameters.
Unfortunately, numerous studies~\cite{heersurvey,hildasurvey,sciencdirect (http://www.sciencedirect.com/science/article/pii/S0747563211000707)}, reports~\cite{citibank} and anecdotal evidence have consistently found~\cite{heerstudy,http://www.sciencedirect.com/science/article/pii/S0747563211000707} that human-generated data can be incredibly dirty.
These input errors will in the best case, manifest as individual insert errors, and in the worst case, an error to bulk update or delete queries that affect a large number of records.
Overall, data errors are estimated to cost the US economy between \$XXX and \$YYY, and user errors in OLTP applications are a potentially large source of such errors.
Consider the following example:


\begin{example}[Tax bracket adjustment]\label{ex:taxes}
In 2012, there were nearly 90k local governments in the United States.  Each manages tax rate, regulatory penalties, and other financial information use one or more form-based accounting systems~\cite{https://tax.thomsonreuters.com/new-systems-for-government-revenue-management/ }.  Updates to the financial information are ultimately managed through manual form entry and susceptible to error.  Most recently in 2015, a “tax mistake, affecting taxpayers in 37 states” affected thousands of lower-income Americans~\cite{http://www.nytimes.com/2015/02/21/us/incorrect-tax-information-health-insurance.html}.  
Figure~\ref{fig:example} shows a simplified tax rate adjustment scenario and highlights how a single error to the tax rate attribute in an update query can propagate and affect record attribute values throughout the database.
\end{example}

Although the federal government found the root cause in this high profile case, we believe this is the exception rather than the rule.  In these applications, data errors are typically identified and reported by individual customers to a customer services-type department.  Even in large companies, these departments will often not have the capacity nor the ability to investigate the errors more broadly.  Instead, the standard course of action is to correct mistakes on a case-by-case basis for each complaint. As a result, unreported errors remain in the database for a long time, or they never get corrected, and their cause becomes harder to trace as further queries modify the database.   There is a need for tools to identify the anomalous queries (root causes) that introduced such data errors when given examples of errors in the current database.   

Unfortunately, this problem has the following important characteristics that render it very difficult, and unsuitable for existing techniques:

\begin{description}[leftmargin=*, topsep=0mm, itemsep=0mm]
\item[Obscurity.] Handling data errors directly often leads to partial fixes that further complicate the eventual diagnosis and resolution of the problem. For example, a transaction implementing a change in the state tax law updated tax rates using the wrong rate, affecting a large number of consumers. This causes a large number of complaints to a call center, but each customer agent usually fixes each problem individually, which ends up obscuring the source of the problem.
\item[Large impact.] Erroneous queries cause errors at a large scale. The potential impact of the errors is high, as manifested in several real-world cases~\cite{Yates10, Grady13, sakalerrors}. Further, errors that remain undetected for a significant amount of time can instigate additional errors, even through valid updates. This increases both their impact, and their obscurity.
\item[Systemic errors.] The errors created by bad queries are \emph{systemic}: they have common characteristics, as they share the same cause. The link between the resulting data errors is the query that created them; cleaning techniques should leverage this connection to diagnose and fix the problem. Diagnosing the cause of the errors, will achieve systematic fixes that will correct all relevant errors, even if they have not been explicitly identified.
\end{description}

There are two approaches towards data errors.  The first is a reactive approach that uses combination detection and repair.  The detection step uses techniques such as human reporting, outlier detection~\cite{}, or integrity constraint violations~\cite{} to identify a candidate set of errors in the dataset.  
The repair step then uses techniques such as human experts, denial constraints or value imputation to predict and replace the incorrect fields with the correct values.   There are a number of issues with this approach, because it ultimately targets the symptom (incorrect database state) rather than the underlying cause (incorrect queries).  First, it can be expensive to (manually) clean the records individually, particularly if the error as spread to many records in the database.  In addition, automated data repair is not perfect~\cite{paolo study} and may additionally introduce errors.  Finally, unless {\it all of the errors are correctly repaired}, this process obscures root cause and makes it harder to identify other data affected by the bad query.

The second approach attempts to prevent data errors by guarding against erroneous updates.  For example, integrity constraints~\cite{Khoussainova2006} reject some improper updates, but only if the data falls outside rigid, predefined ranges.  In addition, data entry errors such as the tax example above will likely satisfy database integrity constraints and not be rejected.  Certificate-based verification~\cite{Chen2011} is less rigid, but it is impractical and non-scalable as it requires users to answer challenge questions before allowing each update.

In contrast to these existing approaches, we would like a solution that can identify the most  queries that most likely introduced the errors.  
In addition, it should be fast enough to integrate into future data cleaning and analysis systems~\cite{wisteria, hildacleaning}
in essence, want to take the query lineage of the errors that have been identified, 

In this paper, we present the \sys, a diagnosis and repair system for data errors caused by errors in DML queries.  In contrast to existing data cleaning approaches that directly correct data errors, \sys generates diagnoses that explain how user-specified data errors were introduced into the database by identifying the most likely query-based sources of these errors.  Alongside these diagnoses, \sys proposes ways to repair the erroneous query that can fix the specified errors, as well as potentially identify and fix the additional errors in the data that would have otherwise remained undetected.  In addition, the system can generate diagnoses and repairs in interactive time for OLTP applications  exemplified by OLTPBench workloads.  This shows promise for integrating it into future interactive data cleaning and analysis systems~\cite{paolo,wisteria,hildacleaning}.  

In summary, our contributions include:
\begin{itemize}[leftmargin=*, topsep=0mm, itemsep=0mm]      
    \item formalize and introduce the problem of “query explanation and repair”
    \item introduce a novel MILP formulation that can address the above techincal challenges, as well as a series of optimizations that enable it to run over tens of thousands of queries, XXX records, in single digit seconds.
    \item thorough study~\cite{techreport} of the data and workload characteristics that affect \sys's performance and repair quality.  We find that
    \item show scalability on common OLTP application benchmarks such as TPCC and XXX
\end{itemize}

We are excited because \sys is first system that both diagnoses and repairs data errors by using query histories.
We envision extending our techniques to support CRUD-type program code to support end to end application debugging.




\section{Experiments}
\label{sec:experiments}
 \begin{figure*}[!htb]
    \vspace*{-.1in}
    \centering
    \begin{subfigure}[t]{.49\textwidth}
    \includegraphics[width = .99\columnwidth]{figures/attr_time}
    \vspace*{-.1in}
    \caption{\# of attributes vs time ($N_D = 100$).}
    \label{f:attr} 
    \end{subfigure}
    \begin{subfigure}[t]{.49\textwidth}
    \includegraphics[width = .99\columnwidth]{figures/attr100_time}
    \vspace*{-.1in}
    \caption{Database size vs time ($N_a = 100$).}
    \label{f:attr100} 
    \end{subfigure}
    \vspace*{-.1in}
    \caption{For datasets with many attributes, the optimizations result in significant improvements.}
    \label{f:database}
  \end{figure*}


In this section, we carefully study the sensitivity of the
basic, optimized and incremental variations of the \sys algorithm 
to changes in the database and query log characteristics.
Due to the difficulty of collecting corrupt query logs from active deployments, 
our goal instead is to understand these trade-offs in
controlled synthetic scenarios, as well as use common OLTP query workloads
from two widely used database benchmarks.

To this end, we first focus on the case of single query corruptions and evaluate
the optimized \sys incremental algorithm on two OLTP benchmarks (Section~\ref{sec:experiments:benchmar}) and find that \sys can return high quality repairs in interactive speeds.
We then evaluate the variations of the incremental algorithm in a wide range of controlled database and query log settings (Section~\ref{sec:experiments:inc})
Finally, we evaluate the incremental and basic algorithms on more challenging settings of incomplete complaint sets and multiple query corruptions and discuss why the latter setting is fundamentally difficult (Section~\ref{sec:experiments:synth}).
All experiments were run on 12x2.66 GHz  machines with 16GB RAM running IBM CPLEX~\cite{cplex2014v12} as the MILP solver on CentOS release 6.6.
In the following text, \sys refers to the incremental algorithm that uses all slicing optimizations.


% the performance and accuracy
% characteristics of the basic MILP-based repair algorithm, 
% slicing-based optimizations that improve the latency of the system, 
% and the incremental algorithm for single query corruptions. 

% \looseness -1
% \xlw{
% To this end, our experiments are organized as follows: First, 
% we compare the basic and incremental MILP algorithm against the different optimizations 
% to highlight the value of different optimizations and the limitations of the basic approach. We then evaluate \sys using established database transaction benchmarks from OLTP-bench~\cite{difallah2013oltp}:
% TPC-C~\cite{tpcc} and TATP~\cite{tatp}. In addition, we study the behavior of \sys using synthetic data sets and show that \texttt{UPDATE} queries are particularly difficult to repair and focus solely
% on different types of \texttt{UPDATE}-only workloads to understand how \sys responds to different parameter settings.
% We end with a discussion of \sys's capability in solving problems in more complex settings.}
% All experiments were run on 12x2.66 GHz  machines with 16GB RAM running IBM CPLEX~\cite{cplex2014v12} as the MILP solver on CentOS release 6.6.
% 






\subsection{Experimental Setup}


\iffalse
\begin{table}[t]\small
  \centering
  \begin{tabular}{@{}cll@{}}
  \toprule
  {\bf Param} & {\bf Description} & {\bf Default} \\ \midrule
  $V_d$  & Domain range of the attributes  & $[0, 100]$ \\
  $N_D$  & \# tuples in final database & $1000$ \\
  $N_a$  & \# attributes in database & $10$ \\
  $N_w$  & \# predicates in \texttt{WHERE} clauses & $1$ \\
  $N_s$  & \# \texttt{SET} clauses & $1$ \\
  $N_q$  & \# queries in query log & $300$ \\
  $idx$  & Index of corrupted query & $\{0, 25, 50,$ \\
         & (backwards from most recent) & $100, 200, 250 \}$ \\ 
  $r$    & Range size of \texttt{UPDATE} queries & 8 (tuples) \\
  $s$    & Zipf $\alpha$ param of query attributes, & $1$ \\ \bottomrule \end{tabular}
  \caption{Experimental Parameters}
  \label{t:params}
\end{table}
\fi


\iffalse
  \begin{table}[t]\small
    \centering
    \begin{tabular}{@{}cl@{}}
    \toprule
    {\bf Param} & {\bf Description} \\ \midrule
    $p$ & Precision: \% of repaired tuples that are correct. \\
    $r$ & Recall: \% of full complaint set repaired.\\
    $t_{total}$ & End-to-end execution time \\ 
    $d_{measure}$ & \red{Some sort of distance measure} \\ \bottomrule \end{tabular}
    \caption{Metrics Compared}
    \label{t:metrics}
  \end{table}
\fi



For each of our experiments we generate and corrupt a query log. 
We execute the original and corrupt query logs on an initial (possibly empty) database,
perform a tuple-wise comparison between the resulting database states to generate a true complaint set,
and simulate incomplete complaint sets by removing a subset of the true complaints.
Finally, we execute the algorithms and compare the repaired query log with the true query log, as well as the repaired and true
final database states, to measure performance and accuracy metrics.
Performance is measured as wall clock
time between submitting a complaint set and the system terminating after retrieving a valid repair.  
Accuracy is reported as the repair's precision (percentage of repaired tuples that were correctly fixed), 
recall (the percentage of the full complaint set that was repaired), 
and F1 measure (the harmonic mean of precision and recall).
We report the average across 20 runs.
We describe the experimental parameters in the context of the datasets and workloads below.





\stitle{Synthetic:} \label{sec:syntheticgen}
We generate an initial database of $N_D$ random tuples.  
The schema contains a primary key $id$ along with $N_a$ attributes $a_1\ldots a_{N_a}$, whose values are integers picked from $[0, V_d]$ uniformly at random.
We then generate a sequence of $N_q$ queries.
The default setting for these parameters are: $N_D = 1000, N_a = 10, V_d = 200, N_q = 300$.


\texttt{UPDATE} queries are defined by a SET clause that assigns an attribute a $Constant$ or $Relative$ value,
and a WHERE clause can either be a $Point$ predicate on a key, or a $Range$ predicate on non-key attributes:

{\scriptsize
\begin{verbatim}
 SET Clause:                WHERE Clause:
  Constant: SET (a_i=?), ..   Point: WHERE a_j=? & ..
  Relative: SET (a_i=a_i+?)   Range: WHERE a_j in [?,?+r] & ..\end{verbatim}}
\noindent where \verb|?|$\in [0, V_d]$ is random and \verb|r| is the size of the range predicate. 
Query selectivity is by default $2\%$ (\verb|r|$=4$).
Note that a range predicate where \texttt{r = 0} is distinct from a $Point$ predicate due to the non-key attribute.
The WHERE clauses in \texttt{DELETE} queries are generated in an identical fashion, while
\texttt{INSERT} queries insert values picked uniformly at random from $V_d$.
By default, we generate \texttt{UPDATE} queries with non-key range predicates and constant set clauses.
  
In addition, the skew parameter $s$ determines the distribution attributes referenced in the \texttt{WHERE} and \texttt{SET} clauses.  
Each attribute in a query is picked from either a uniform distribution when $s=0$ or a zipfian distribution with exponent $s$.
This allows our experiments to vary between a uniform distribution, where each attribute is
equally likely to be picked, and a skewed distribution where nearly all attributes are the same. 

\looseness -1
\stitle{Benchmarks: } We use the TPC-C~\cite{tpcc} and TATP~\cite{tatp} benchmarks.
The former generates the {\it ORDER} table at scale 1 with one warehouse, and uses the queries that modify the {\it ORDER} table. 
We execute a log of 1500 queries over an initial table containing 6000 tuples.  
$1837$ queries are \texttt{INSERT}s and the rest are \texttt{UPDATE}s. 
The latter TATP workload simulates the caller location system. 
We generate a database from {\it SUBSCRIBER} table with 5000 tuples and $1500$ \texttt{UPDATE} queries.
Both setups were generated using the OLTP-bench~\cite{difallah2013oltp}. 
We introduce a single corruption, and vary corrupted query's index from the most recent query $q_N$ to $q_{0}$.



\stitle{Corrupting Queries:} We corrupt query $q_i$ by replacing it with a randomly
generated query of the same type based on the procedures described above.
To standardize our procedures, we selected a fixed set of indexes $idx$
that are used in all experiments. 




\subsection{Benchmarks}
\label{sec:experiments:benchmark}
In this experiment, we vary the location of a single corrupt query in the TPC-C and TATP benchmark query logs and report \sys's performance;
in all runs, \sys achieves an F1 score of $1$.
Figure~\ref{f:tpcctatp} shows that \sys can generate a repair for TPC-C and TATP within milliseconds and tens of seconds, respectively.
The key reason is that in practice, each query affects a small set of records and results in a very small complaint set---$1-2$ on average.
Tuple and query slicing are also able to aggressively reduce the total number of constraints to $<100$ constraints on average.

\sys can repair TPC-C  queries are predominantly \texttt{INSERT}s, which \sys can solve within milliseconds. 
In contraist, TATP only contains \texttt{UPDATE}s, which are harder to solve than \texttt{INSERT} queries and thus lead to higher execution time compared with TPC-C queries.
Note that these experiments stripped out read-only queries from the workload, which account for \ewu{XXX}\% of the queries.
Finally, \sys repairs Example~\ref{ex:taxes} in Figure~\ref{fig:example} within 35 milliseconds. 

\begin{figure}[h]
\centering
  \includegraphics[width = .75\columnwidth]{figures/benchmark_time}
  \vspace*{-.2in}
  \caption{\sys quickly produces repairs for OLTP workloads.}
  \label{f:tpcctatp} 
\end{figure}

{\it Takeaways: many workloads in practice are dominated by \texttt{INSERT} and point \texttt{UPDATE} queries (ignoring the dominant percentage of read-only queries).  
In these settings, \sys is very effective at reducing the number of constraints and can derive repairs with near-interactive latencies.}





  \begin{figure*}[!htb]
    \hspace*{-.1in}
    \centering
     \begin{subfigure}[t]{.33\textwidth}
      \includegraphics[width = .99\columnwidth]{figures/indelup_time}
      \caption{Performance for different query types.}
      \label{f:indelup_time} 
    \end{subfigure}
    \begin{subfigure}[t]{.33\textwidth}
      \includegraphics[width = .99\columnwidth]{figures/pointrelv_time}
      \caption{Performance of diff. query clause types.}
      \label{f:qidx_time} 
    \end{subfigure}
    \begin{subfigure}[t]{.33\textwidth}
      \includegraphics[width = .99\columnwidth]{figures/where_time}
      \caption{Query dimensionality vs time.}
      \label{f:where_time} 
    \end{subfigure}
    \caption{
    Performance and accuracy can be sensitive to some data and workload parameters, such as query skew, clause types, and dimensionality.  Notably, \sys can handle high false negative rates when the error resides in recent queries.}
  \end{figure*}



\subsection{Sensitivity of the Incremental Algorithms}
\label{sec:experiments:incsec}

This subsection evaluates the efficacy of using each slicing optimization on the incremental algorithm by 
varying the characteristics of the database and query log.  

\subsubsection{Sensitivity to Database Properties}

The following tow experiments compares different combinations of the slicing optimizations \emph{tuple/query/attr} under varying database size and schema size settings.  
Each facet (subplot) in Figure~\ref{f:database} represents the location of the corruption as $q_{50, 200, 300}$.
We report performance and omit accuracy numbers because the F1 for all settings is nearly 1.

% \looseness -1
% The following set of experiments are designed to establish the rationale for 
% the settings in the subsequent experiments.  
% \xlw{
% Specifically, we compare different slicing-based optimizations of incremental repair (\emph{$Inc_1$})
% as the hardness of database (number of attribute or database size) increases in the context of a single
% corrupted query: \emph{inc1-all} as the incremental repair with all slicing optimization and \emph{inc-tuple/query/attr} as
% the incremental repair with the corresponding tuple, query, and attribute slicing optimization. In addition, we omit the accuracy figures as the F1 score for all settings is nearly 1.}

\stitle{Increasing \# of Attribute :} \looseness -1
We first vary the number of attribute ($N_a \in [10, 500]$) under a fixed database size $N_D = 100$.
As shown in Figure~\ref{f:attr}, when the number of attribute in a table is small (e.g., $N_a=10$) or when the corruption is recent (e.g., $q_{200, 300}$), then all optimizations appear identical. 
However, increasing the number of attribute exhibits a larger benefit for query and attribute slicing (up to $6.8\times$ reduction compared to tuple-slicing).
When the table is wide ($N_a = 500$), applying all optimizations ($inc_1-all$) is $40\times$ faster than tuple-slicing alone.  


\stitle{Increasing Database Size:} \looseness -1
We now vary the database size ($N_D \in [100,5000]$) with a large number of attributes ($N_a = 100$).
We fix the number of complaints by decreasing the query selectivity in proportion to $N_D$'s increase---the specific mechanism to do so did not affect the findings.
Figure~\ref{f:attr100} shows that the costs are relatively flat until the corruption occurs in an old query ($q_{50}$).  
In addition, we find that the cost is highly correlated with the number of candidate queries that are encoded in the MILP problem.
The increase in cost despite tuple-slicing is due to the increasing number of candidate queries in the system; 
we believe this increasing trend is due to the solver's ability to prune constraints that correspond to queries that clearly will not affect the complaint set---an implicit form of query slicing.  
Applying attribute-slicing supercedes this implicit optimization and results in a flat curve, 
while query-slicing explicitly reduces the number of candidate queries in proportion with the database size, and leads to the increasing trend.
Ultimately, combining all three optimizations outperforms tuple-slicing by $2-4\times$.

\smallskip
\textit{Takeaways: we find that the performance of the different repair algorithm 
heavily depends on the property of the datasets---in particular, the number of attributes and the number of tuples in the database. Tuple slicing is essential to solve general problems. Attribute and query slicing show significant gain for datasets with large number of attributes. }

\ewu{Need to introduce QFix again as setup for the subsequent experiments}
\xlw{Note that in both experiments, we omit the results of algorithms without applying tuple-slicing optimization as the these algorithms failed to scale over most of the problems. In the following experiments, we focus on studying the performance of a single \sys setting---incremental with tuple-slicing---on a narrow table setting that contains $N_a = 10$ attributes, and a single corrupt query in the query log.}


\subsubsection{Sensitivity to Query Log}
\label{sec:experiments:synth}
\xlw{
The following set of synthetic experiments evaluates \sys (incremental with tuple-slicing)'s performance while increase the hardness of queries by varying numerous workload parameters such as query type, query complexity, and log size $N_q$. Again, we skip the accuracy figures as \sys achieves near 1 F1 score in the following set of experiments. }

\iffalse
\emph{Database Size:} \looseness -1
Figure~\ref{f:dbsize_time} varies the database size ($N_D \in [100, 100k]$), and fixes query output cardinality and complaint set size in the same way as the previous scalability experiment.
In contrast to the previous experiment, the scalabality curve is nearly flat for both corruption query indices.
The reason is because the solver's implicit pruning optimization is less effective when there are only $10$ attributes: Every query is likely to touch an attribute that affects the complaint set.
We verified this by applyng query-slicing to the same setting, and found far fewer queries were pruned compared to the previous experiment.
It takes less than a minute to perfectly repair the recent corruption $q_{200}$,
and around $4$ minutes for the older corruption $q_{50}$, even as the database size increases.
At smaller database sizes, the randomness in the workload generator leads to variability in the size of the complaint set, and ultimately a larger and more difficult MILP problem.
The exponential relationship between solver time and problem difficulty results in the higher average latency for $q_{50}$.
\fi


\stitle{Query Type: }\label{sec:indelup}
This experiment evaluates the \sys 
($inc_1-tuple$) on \texttt{INSERT}, \texttt{DELETE}, or \texttt{UPDATE}-only workloads.
We increase the number of queries from $1$ to $200$ and corrupt the oldest query in the log.  
Figure~\ref{f:indelup_time} shows that while the cost of repairing \texttt{INSERT} workloads
remains relatively constant, the cost for \texttt{DELETE}-only and \texttt{UPDATE}-only workloads increase as 
the corruption happens earlier in the query log---and a much faster rate for \texttt{UPDATE} queries.

\ewu{Probably not needed at this point.  Maybe instead say "This is why we have focused on UPDATE queries in most of our experiments.}  We observe that \texttt{UPDATE} queries translate into more undetermined variables than other query types, and are significantly more expensive to repair. 
\iffalse
Based on these results, we focus on the incremental algorithm $inc_1$ and the more difficult \texttt{UPDATE}-only workloads in the rest of the experiments.
\fi

\smallskip
\stitle{Query Clause Type: }
So far, we have focused on \texttt{UPDATE} queries with constant set clauses and range predicates.  
Figure~\ref{f:qidx_time} individually varies each clause and compares against {\it Constant/Point} and {\it Relative/Range} queries. 
The x-axis varies the index of the corrupted query between $q_1$ and $q_{249}$.
We find that point predicates and constant set clauses are easier to solve than range predicates and relative set clauses, respectively.
The reason for the former pair is because range predicates double the number of undetermined variables as compared to point queries.  
In addition, point queries are on key attributes, thus further reduces the possible search space.  
We believe the latter pair is because the constant set clauses break the causal relationship between input and output records for the overwritten values.
This both simplifies the difficulty of the constraint problem, and reduces the total number of constraints.


\smallskip
\stitle{Predicate Dimensionality:}
Figure~\ref{f:where_time} varies the dimensionality of the update queries by increasing the number of predicates in the \texttt{WHERE} clause, while keeping the query cardinality constant.
The cost increases with the dimensionality because each additional predicate is translated into a new set of constraints and undetermined variables, increasing the problem complexity.
\ewu{Mention potential solutions to this issue}


\smallskip
\textit{Takeaways: we find that problems with \texttt{UPDATE}-workloads are significantly harder than problems with other types of queries. In addition, the performance of \sys also closely related to the complexity and the property of queries. 
\sys is able to solve hard problems (with ~$200$ \texttt{UPDATE}-only queries) in seconds or minutes---particularly if the error is recent. }


\subsection{Solving Hard Problems}
\label{sec:experiments:hardprob}

\ewu{Is this setion for "solving hard problems", or examining the limitations of basic on multi-query errors?} \xlw{This section is about examining \sys's limitations on two scenarios: incomplete complaint set information \& scalability over multi-query errors. Maybe we can change to section title as: Examining Limitations? or any other suggestions?}

In this section, we further study the performance of \sys in solving hard problems. In particular, we conduct two sets of experiments: the first experiment evaluate \sys with incomplete complaint sets; the second illustrate \sys's capability in handling multiple errors.
 
\stitle{Incomplete Complaint Set:} \looseness -1
The first experiment (Figures~\ref{f:falsenegative_time} and~\ref{f:falsenegative_acc}) varies the fase positive rate in incomplete complaint sets.
We increase the rate from $0$ ($0\%$ missing in the complaint set) to $.75$ ($75\%$ are missing).  
We find that reducing the size of the submitted complaint set naturally improves the repair performance,
however the repair quality, both precision and recall in Figure~\ref{f:falsenegative_acc}, may suffer if the corruption occured in a very old query. 
This is expected because \sys targets reported complaints, thus unreported complaints may easily be missed and lead to low recall.
In addition, despite the refinement step of tuple-slicing, the repair may over generalize and ``fix'' the wrong records, leading to low precision.


    
\begin{figure}[!htb]
  \hspace*{-.1in}
  \centering
    \begin{subfigure}[t]{.33\textwidth}
      \includegraphics[width = .99\columnwidth]{figures/noise_fn_time} 
      \caption{False negatives vs time.}
      \label{f:falsenegative_time} 
    \end{subfigure}
    \\
    \begin{subfigure}[t]{.33\textwidth}
      \includegraphics[width = .99\columnwidth]{figures/noise_fn_acc}
      \caption{False negatives vs accuracy.}
      \label{f:falsenegative_acc} 
    \end{subfigure}
    \vspace*{-.1in}
    \caption{Our analysis highlights limitations of \naive, the value of tuple-slicing, and the high cost of \texttt{UPDATE} queries.}
  \end{figure}

\iffalse
\emph{Skew:} We now study the effects of attribute skew on the algorithms.
We increase the skew parameter from $0$ (uniform) to $1$ (nearly every attribute is $A_0$) 
and find a reduction in latency (Figure~\ref{f:skew_time}).
We believe the reason is because increasing the skew focuses the query predicates over a smaller set of logical attributes, 
and increases the number of constraints placed on each of the logical attributes used in the query log.  
Each of these constraints reduces the search space of allowable values for that attribute, and thus simplifies the MILP problem.
This result suggests that \sys may be well suited for many transaction systems that naturally exhibit query skew.
Note that the overall number of constraints in the problem is the same, only their distribution over the query attributes has changed.
\fi


\stitle{Multiple Corrupt Queries:} \looseness -1
In the second experiment, we compare the basic approach (\naive) against 
each slicing optimization individually($basic-tuple, basic-attr, basic-query$).  
We use the default settings with $N_D = 1000$ tuples and a sequence of \texttt{UPDATE} queries.
We generate query logs in 5 different sizes $N_q\in \{10, 20, 30, 40, 50\}$ and corrupt 
every tenth query starting from oldest query $q_1$,
up to $q_{41}$.  For example, when the $N_q = {30}$, we corrupt 3 queries: $q_{1,11,21}$. 
We find that the number of queries greatly affects both the scalabality (Figure~\ref{f:multi_time}) 
and the accuracy (Figure~\ref{f:multi_acc}) of the algorithms. Specifically, as the number increases,
the number of possible assignments of the MILP parameters increases exponentially and the solver often takes
longer than our experimental time limit of $1000$ seconds and returns an infeasibility error.  
This is a predominant reason why the accuracy degrades past $30$ queries.  For example, 
when $40$ queries are involved (with $4$ corruptions) 
and we ignore the infeasible executions, the average execution time is $300$ seconds
and the precision and recall are greater than $0.94$.  Unfortunately, with $50$ queries ($5$ corruptions),
all runs exceed the time limit and return infeasibility.

  \begin{figure}[!htb]
  \hspace*{-.1in}
  \centering
    \vspace*{-.2in}
    \begin{subfigure}[t]{.33\textwidth}
    \includegraphics[width = .99\columnwidth]{figures/multi_time}
    \caption{Performance for multiple corruptions.}
    \label{f:multi_time} 
    \end{subfigure}
    \\
        \begin{subfigure}[t]{.33\textwidth}
    \includegraphics[width = .99\columnwidth]{figures/multi_pr}
    \caption{Accuracy for multiple corruptions.}
    \label{f:multi_acc} 
    \end{subfigure}
        \vspace*{-.1in}
    \caption{Our analysis highlights limitations of \naive, the value of tuple-slicing, and the high cost of \texttt{UPDATE} queries.}
  \end{figure}

\iffalse
\stitle{Single Corrupt Query:}
In this experiment, we evaluate the efficacy \sys with tuple slicing and incremental optimization
in the special case when one query has been corrupted in a much larger query log. 
We compare \incremental without tuple slicing ($inc_1$) against tuple slicing at 
different batching levels of 1, 2, 8 ($inc_1-tuple; inc_2-tuple, inc_8-tuple$). 
Recall from Section~\ref{sec:incremental} that $inc_k$ parameterizes $k$ consecutive queries in each batch until a repair is found.
Figure~\ref{f:singlequeryinc_time} highlights the scalability limitation of the incremental 
algorithm without tuple-slicing: with 50 queries $inc_1$  easily exceeds the 1000s limit.   
The tuple-slicing scales significantly better (nearly $200\times$ faster), however
the accuracy severely degrades when $k>1$.  
The primary reason is because of infeasibility errors---the MILP problem is much harder, and fails to find a repair.  
This is highlighted by the symmetry between the precision and recall curves.  
A secondary reason is beause the refinement step of  tuple slicing may not generate a fully correct repair and generalize incorrectly.
This is why the precision curve is lower than the recall curve for $inc_8-tuple$.
\fi


\smallskip
\emph{Takeaways: We observe \sys is able to solve recent errors (e.g., errors in most recent 100 queries) efficiently and effectively even with very limited information. Also, we find that basic, even with slicing optimizations, has severe scalability
limitations due to the large number of undetermined values-this is unsurprising as MILP constraint solving is an NP-hard
problem.
}


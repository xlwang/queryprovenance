
\documentclass{vldb}
\usepackage{balance}  
\usepackage{graphicx} 
\usepackage[hyphens]{url}      
\usepackage{amsmath}
\usepackage{color}
\usepackage{cancel}
\usepackage{listings}
\usepackage[normalem]{ulem}
\usepackage{graphicx}
\usepackage{subcaption}
\setlength{\abovecaptionskip}{10pt plus 3pt minus 2pt}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[ruled,vlined,algonl,boxed]{algorithm2e}
\usepackage{algorithmic}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{xspace}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage[T1]{fontenc}
\usepackage{fancyvrb}


\usepackage[nocompress]{cite}
\usepackage{microtype}
\usepackage[section]{placeins}

\makeatletter
\def\url@leostyle{
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\bf\ttfamily}}}
\makeatother
\urlstyle{leo}

\makeatletter
\def\@copyrightspace{\relax}
\makeatother

\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

\newtheorem{definition}{Definition}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{remark}[definition]{Remark}
\newtheorem{corollary}[definition]{Corollary}
\newtheorem{claim}[definition]{Claim}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{heuristic}[definition]{Heuristic}
\newtheorem{example}[definition]{Example}
\newtheorem{dimension}{Dimension}
\newcounter{prob}
\newtheorem{problem}[prob]{Problem}
\newtheorem{conjecture}[definition]{Conjecture}
\newtheorem{reduction}[definition]{Reduction}
\newtheorem{property}[definition]{Property}
\newtheorem{axiom}[definition]{Axiom}











\usepackage[pdftex]{hyperref}
\hypersetup{
  colorlinks=false,
  linkcolor=darkred,
  citecolor=darkgreen,
  urlcolor=darkblue
}



\widowpenalty=10000
\clubpenalty=10000


\definecolor{light-gray}{gray}{0.95}
\definecolor{mid-gray}{gray}{0.85}
\definecolor{darkred}{rgb}{0.7,0.25,0.25}
\definecolor{darkgreen}{rgb}{0.15,0.55,0.15}
\definecolor{darkblue}{rgb}{0.1,0.1,0.5}
\definecolor{blue}{rgb}{0.19,0.58,1}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}
\newcommand{\darkred}[1]{\textcolor{darkred}{#1}}
\newcommand{\darkgreen}[1]{\textcolor{darkgreen}{#1}}
\newcommand{\darkblue}[1]{\textcolor{darkblue}{#1}}

\makeatletter
\setlength{\@fptop}{0pt}
\makeatother



\widowpenalty 10000
\clubpenalty 10000


\newcommand{\papertext}[1]{#1}
\newcommand{\techreport}[1]{#1}


\newcommand{\alex}[1]{\noindent{\color{darkgreen}{[Alexandra: #1]}}}
\newcommand{\xlw}[1]{\noindent{\color{blue}{Xiaolan: #1}}}
\newcommand{\ewu}[1]{\noindent{\color{red}{EWu: #1}}}

\newcommand{\xxx}[1]{{\fontsize{13pt}{13pt}\selectfont\textcolor{red}{#1}}}
\newcommand{\codesize}{\fontsize{7}{8}}
\newcommand{\stitle}[1]{\vspace{0.5em}\noindent\textbf{#1}}
\newcommand{\calF}[0]{$\cal{F}$}

\newcommand{\ind}{\hspace{\algorithmicindent}}

\newcommand{\deprecate}[1]{\noindent{\color{light-gray}{#1}}}

\newcommand{\prob}{{\sc Log-Corruption}\xspace}
\newcommand{\exact}{{\sc EXACTSOL}\xspace}
\newcommand{\qfix}{{\sc SingleQueryFix}\xspace}
\newcommand{\density}{{\sc DENSITY}\xspace}


\newcommand{\milpall}{\textsc{MILP-NAIVE}\xspace}
\newcommand{\milptuple}{\textsc{MILP-COMPL}\xspace}
\newcommand{\milptuplestopearly}{\textsc{MILP-COMPL-STOPEARLY}\xspace}
\newcommand{\milpadvtuple}{\textsc{MILP-ADV-TUPLE}\xspace}
\newcommand{\milpadvall}{\textsc{MILP-ADV-ALL}\xspace}
\newcommand{\heurstic}{\textsc{HEURISTIC}\xspace}

\pagenumbering{arabic}

\makeatletter
\def\maketag@@@#1{\hbox{\m@th\normalfont\normalsize#1}}
\DeclareRobustCommand*\textsubscript[1]{
          \@textsubscript{\selectfont#1}}
        \def\@textsubscript#1{
          {\m@th\ensuremath{_{\mbox{\fontsize\sf@size\z@#1}}}}}
\makeatother

\newcommand{\sysname}{\textsc{QueryFix}}
\newcommand{\sys}{QFix\xspace}
\newcommand{\naive}{\emph{basic}\xspace}
\newcommand{\tslice}{\sys-{\it tuple}\xspace}
\newcommand{\qslice}{\sys-{\it query}\xspace}
\newcommand{\aslice}{\sys-{\it attr}\xspace}
\newcommand{\incremental}{\sys-{\it inc}\xspace}
\newcommand{\dt}{DecTree\xspace}


\setlength\floatsep{0.8\baselineskip plus 3pt minus 2pt}
\setlength\textfloatsep{0.9\baselineskip plus 3pt minus 2pt}
\setlength\intextsep{1\baselineskip plus 3pt minus 2 pt}


\begin{document}

\subsection*{Meta Review Details}
\noindent \textbf{R1: Discuss and clarify the soundness of slicing techniques.}

(add text)

\noindent \textbf{R2 Clarify the assumptions, their interaction, and why they are realistic in practice. clarify that the system is not repairing the update queries, but the affected data (i.e., query is not correct and errors can still happen).}

(add text)

\noindent \textbf{R3 Add experiment with larger data size, or clarify why they cannot be done.}

(add text)

\noindent \textbf{R4 Investigate how many of the errors exhibit interesting patterns (such as a later update masking the effect of an earlier erroneous one). If there are already lots of examples of this, compare effectiveness on the "simple" errors with the "complex" ones; otherwise, you should improve the test data generator.}

(add text)

\noindent \textbf{R5 Implementation of a baseline (described in the review) or explain why such baselines would fail in the context, to convince readers that complex patterns can be handled only by this new system.}

(add text)
\subsection*{Reviewer 1 Details}
\noindent \textbf{R1.1 What are the soundness/correctness properties needed for the slicing heuristics to be applicable/useful? It seems they can be useful even if not sound, but showing this is the case on non-synthetic data would help. I appreciate that realistic benchmarks may not be available though.}

(add text)

\noindent \textbf{R1.2 Please correct definition 6.}

(add text)

\noindent \textbf{R1.3 Do the experiments substantiate the claim of scalability to "large" databases of 100k records? If so the experimental evaluation needs to explain this more clearly. If not the introduction must avoid overselling this point.}

(add text)

\noindent \textbf{R1.4 It's also not entirely clear why it is correct to use a "large/unused" value $M^+$ for attributes of deleted tuples. Please spell this out.}

(add text)
\subsection*{Reviewer 2 Details}
\noindent \textbf{R2.1 Given that the paper considers a novel problem, I believe it could trigger quite some follow-up work. However, to do so, I personally think that further technical details or detailed theoretical discussion are necessary. }

(add text)

\noindent \textbf{R2.2 I find the experimental evaluation rather preliminary and would appreciate experiments on larger data sets or some real data set.}

(add text)
\subsection*{Reviewer 3 Details}

\noindent \textbf{R3.1 The problem is indeed difficult with several sources of complexity. It is challenging to find the right setting to handle it and assumptions must be put in place to solve it with some degree of confidence. However, right now assumptions are scattered all around the paper, and some of them should probably be revised.
My first suggestion is to collect all the restrictions or hypothesis in a unified discussion and try to assess how close is the ultimate setting to the real world scenarios.} 

(add text)

\noindent \textbf{R3.2 Errors are indeed in the queries, but are ultimately data errors that are considered here (no structural errors in the query). It seems related, existing solutions [73,88, Chalamalla et al SIGMOD 2014, Meliou et al SIGMOD 2011] can be directly applied by modelling the query data as source data + SELECT queries. In fact, queries data can be modelled as sources with a new attribute representing the query id, and the data in this papers is the view in previous work. Once identified the problems in the view (as done in all papers) evidence can be carried back to the sources and mined to find the explanations. If the errors are coming from one query (as assumed in most of the exp in this work), I am confident the query attribute will be identified and the explanation will point to that query. I understand that the implementation of such baselines may go behind the scope of the revision, but I believe it would be great if the author can precisely explain why such baselines would fail in their context. ( i suspect the reason is discussed in point 5). }

In this paper, \sys solves two problems: 1. it finds the root reason(s) in the query history that causing database errors, and 2. it fixed the incorrectness by proposing a query log repair. Existing works mentioned above study highly corrected problems: in general, they all target at explaining erroneous or undesired query results by tracing back data source input and reporting either particular input tuples or common input patterns. Although none of them could address both problems that handled by \sys, we explicitly study their ability in identifying errors in the query history (the 1st problem solved by \sys). 

In order to use these works to diagnose query history, we first convert the \textsc{Optimal Diagnosis} problem in Definition 4 as \textsc{Source data + SELECT queries} problem:
Given database states $D_0$ and $D_n$, a query log $\mathcal{Q} = \{q_i\}$ such that $\mathcal{Q}(D_0) = D_n$, and a desired database state $D_n^*$ with no errors, we model the \textsc{Source data + SELECT queries} problem by creating a table $P$ for under-determined parameters in $\mathcal{Q}$ and table $D_0$ as source tables; and a single nested SELECT query $q_n'$ for queries in $\mathcal{Q}$. We demonstrate an abstract example in Example~\ref{fig:example}.
\begin{figure}[t]
The \textsc{Optimal Diagnosis} problem:\\
    \begin{minipage}[t]{0.1\textwidth}
         \vspace{0pt} 
         \centering
        \begin{tabular}{ll}
            \multicolumn{2}{l}{Table $D_0$}\\
            \toprule
            \textbf{A}  & \textbf{B}\\
            \midrule
			 1 & 1 \\
			 2 & 2 \\
			 3 & 5 \\
            \bottomrule
            \\
        \end{tabular}
    \end{minipage}
    \begin{minipage}[t]{0.2\textwidth}
         \vspace{0pt} 
         \centering
        \begin{tabular}{p{26ex}}
            \multicolumn{1}{l}{\emph{Query log}: $\mathcal{Q}$}\\
            $q_1$: \texttt{\small UPDATE T SET B=B+1}\\
            \texttt{\small WHERE A > 2 and a < \sout{5} {\color{red} 3}} \\
            $q_2$: \texttt{\small UPDATE T SET B=B+3}\\
                  \texttt{\small WHERE A > 0 and a < 4} \\
        \end{tabular}
    \end{minipage}
    \begin{minipage}[t]{0.16\textwidth}
         \vspace{0pt} 
         \centering
        \begin{tabular}{ll}
            \multicolumn{2}{l}{Table $D_2^*$}\\
            \toprule
            \textbf{A}  & \textbf{B}\\
            \midrule
			 1 & 1 \\
			 2 & 2 \\
			 3 & {\textbf{6}} \\
            \bottomrule
            \\
        \end{tabular}
    \end{minipage}
The \textsc{Source data + SELECT queries} problem: \\
\begin{minipage}[t]{0.1\textwidth}
         \vspace{0pt} 
         \centering
        \begin{tabular}{ll}
            \multicolumn{2}{l}{Table $D_0$}\\
            (same as above) \\
        \end{tabular}
    \end{minipage}
\begin{minipage}[t]{0.36\textwidth}
         \vspace{0pt} 
         \centering
        \begin{tabular}{llllll}
            \multicolumn{6}{l}{Table $P$}\\
            \toprule
            \textbf{$p_1$}  & \textbf{$p_2$} & \textbf{$p_3$} & \textbf{$p_4$}  & \textbf{$p_5$} & \textbf{$p_6$} \\
            \midrule
			 1 & 2 & 3 & 3 & 0 & 4\\
            \bottomrule
            \\
        \end{tabular}
    \end{minipage}\\
        \begin{minipage}[t]{0.22\textwidth}
         \vspace{0pt} 
         \centering
        \begin{tabular}{p{2ex}p{55ex}}
         \multicolumn{2}{l}{\emph{Queries}: }\\
        $q'_1$: &
        \texttt{\small SELECT $D_0.A$ AS A, $D_0.B$+$p_1$ AS B FROM $P, D_0$ } \\
        & \texttt{\small WHERE $D_0.A$ > $p_2$ AND $D_0.A$ < $p_3$ UNION ALL}\\
        & \texttt{\small SELECT $D_0.A$ AS A, $D_0.B$ AS B FROM $P, D_0$ }\\
        &\texttt{\small  WHERE $\neg$($D_0.A$ > $p_2$ AND $D_0.A$ < $p_3$)} \\
        $q'_2:$ &
         \texttt{\small SELECT $D_1.A$ AS A, $D_1.B$+$p_4$ AS B FROM $P, q_1'\ as\ D_1$ } \\
        & \texttt{\small WHERE $D_1.A$ > $p_5$ AND $D_1.A$ < $p_6$ UNION ALL}\\
        & \texttt{\small SELECT $D_1.A$ AS A, $D_1.B$ AS B FROM $P, q_1'\ as\ D_1$ }\\
        &\texttt{\small  WHERE $\neg$($D_1.A$ > $p_5$ AND $D_1.A$ < $p_6$)} \\
        \end{tabular}
    \end{minipage}
    \caption{An abstract example for R3.2. (For simplicity, we use $q_1'$ for the input sub-query when introducing $q_2'$.)}
\label{fig:example}
\end{figure}

\noindent \textbf{[Roy et al. SIGMOD 2014, Wu et al. VLDB 2013, Chalamalla et al. SIGMOD 2014]} \\
TLDR: These papers quantify responsibility of input data to errors in the output, however responsibility


These papers focus on detecting and summarizing the likely source tuples by propagating ``responsibility'' from annotated output error tuples to sources.[Roy et al. SIGMOD 2014] targets at explaining a set of SELECT query results by a conjunction of predicates on attributes where the SELECT clause in each query is restricted to a single aggregate operator. 
Similarly, [Wu et al. VLDB 2013] explains a group-by SQL query with a single aggregate function. Since there is no obvious mapping between complaints and a particular aggregate function, both algorithms cannot solve the error identification problem. Instead of aggregation results, [Chalamalla et al. SIGMOD 2014] searches for explanations for data quality rule violations. Even though complaints can be expressed as data quality rules, the algorithm does not distinguish the actual input source error (in $D_0$) and query parameter error (in $P$). Thus, in Example~\ref{fig:example}, the explanation $D_0.A = 3$ on table $D_0$ is also a valid explanation. Thus, this work cannot effectively solve the error identification problem either.

\noindent \textbf{[Meliou et al. SIGMOD 2011]} \\
TLDR: The number of disjunctions in the generated SAT expression increases exponentially with respect to the number of queries in the log.  For instance, if the database contained 100 tuples and 10 queries, the number of disjunctions would be approximately XXX.

The View-Conditioned Causality(VCC) work takes a set of input variables $\mathbf{X}$, a transformation $\mathbf{\Phi} =\{\Phi_1, ... , \Phi_m\}$ computing the
output values $\mathbf{z}$, and a ground truth $\hat{\mathbf{z}}$, and detects input variables that are responsible for data errors $\mathbf{z}|\hat{\mathbf{z}}$. 
The \textsc{Source data + SELECT queries} problem can be expressed as a VCC problem where cells in $P$ form the input variables $\mathbf{X}$ and each tuple $t_i$ in $D_0$ and the SELECT query $q_n'$ is translated into a lineage expression $\Phi_i$. In each lineage expression $\Phi_i$, tuple values in $D_0$ and $D_n^*$ are further converted into constant variables. By doing so, only variables in $P$ will be selected to explain data errors. In Example~\ref{fig:example}, lineage expression $\Phi_3$ for tuple $t_3$ is as following:
{\small
\begin{eqnarray*}
&((3 > p_2 \wedge 3 < p_3) \wedge (3 > p_5 \wedge 3 < p_6) \wedge (5 + p_1+p_4 = 6)) &\\
&\vee ((3 > p_2 \wedge 3 < p_3) \wedge \neg(3 > p_5 \wedge 3 < p_6) \wedge (5 + p_1= 6))& \\
&\vee (\neg(3 > p_2 \wedge 3 < p_3) \wedge (3 > p_5 \wedge 3 < p_6) \wedge (5 + p_4= 6))& \\
&\vee (\neg(3 > p_2 \wedge 3 < p_3) \wedge \neg(3 > p_5 \wedge 3 < p_6) \wedge (5 = 6))&
\end{eqnarray*}
}
Each row corresponding to the satisfactory condition for where clauses in $Q$. The first row, for example, means the where condition is true for both $q_1$ and $q_2$. The ground truth output values for $\mathbf{\Phi}$ are all set to \textit{True}. By solving the VCC problem, one can identify incorrect parameters in the query log. However, as you may notice, the lineage expressions for tuples in $D_0$ are composed by disjunctive sub-expressions with exponential cardinality to the number of queries in $Q$. Since the execution efficiency is highly correlated to the complexity of lineage expressions, VCC solves the error detection task but cannot solve it efficiently. 

To conclude, none of these highly correlated works could handle both problems addressed by \sys: [Meliou et al. SIGMOD 2011] solves the error detection problem with low efficiency; [Chalamalla et al. SIGMOD 2014] may solve the error detection problem but fail to distinguish data source errors and query errors; [Roy et al. SIGMOD 2014] and [Wu et al. VLDB 2013] restricted to aggregations, thus cannot solve either problems.

\noindent \textbf{R3.3 Most of the technical contributions are on the scalability aspect, but relatively small datasets are tested. I suggest to explicitly test the scalability until the bottlenecks of the solutions become clear (the MILP module?)}

(add text)

\noindent \textbf{R3.4 I cannot grasp from the experiments section if the authors are counting the number of errors correctly identified or correctly fixed. Usually these are considered two different metrics in data repair. For example, given the problem of income >= 85700, do they consider it correctly fixed with any value that lead to repair of the identified problems or they require to get exactly 87500? 
I believe it must be the former as I don’t see how they can always recover the correct value in general. If this is the case, this clearly explain the big drop in precision and recall when the complain set is incomplete. Unfortunately this implies that even complete complain sets do NOT guarantee that the query is correctly fixed and future errors will not occur (e.g. the system may discover 87200 as the correct fix, it covers all case, but it does not cover new data)}

(add text)

\noindent \textbf{R3.5 I believe the way that errors are introduced can be improved. Randomly introducing errors do not guarantee that complex patterns (such as the one in Figure 1) happen in the test datasets. In fact, what make the problem challenging is the interaction of the queries. I suggest to have an experiment where this interaction is enforced in the noise injection so that the approach can really show its power wrt to simpler techniques (such as point 2 above). 
Unfortunately having some guarantee on the way that errors are introduced can be a challenging problem, a recent paper discusses this topic (Arocena et al, PVLDB 15).}

(add text)


\end{document}

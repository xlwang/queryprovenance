\noindent
\textbf{Comment \#6:} Additional corrections and clarifications

\smallskip

We thank the reviewers for the many detailed comments. We have corrected the
typos, and reworded confusing sentences. We also provide some further notes on
some of the detailed comments below.

\begin{quote}
\reviewer{
p2. Although it is just an example, it does seem like questionable design for
the tax rates for different salary levels to be stored in the query rather
than in the database.
}
\end{quote}

The example is emulating a form-based entry of the tax-rate information, but
we agree that in practice the design would be much more complex. We chose a
substantial simplification to make the example easy to follow and to allow us
to more easily highlight the contributions of our work.

\begin{quote}
\reviewer{
p2. The figures in the example seem wrong (ironically) \dots
}
\end{quote}

Thank you for noting this mistake. We corrected the figure to have the
correct numbers.


\begin{quote}
\reviewer{
p3. This is explained later, but please clarify that the expressions used in
mu and sigma are from a limited language (e.g. arithmetic).
}
\end{quote}

\alex{Have we done anything about this?} \xlw{The explanation and definition is
quite close. We give def and then description. Maybe we should switch the order?}


\begin{quote}
\reviewer{
p3. What does notation ``$D \setminus\{t\}$'' mean (in def 3) if $t = \bot$? No-op? Also,
t* should be allowed to range over $D \cup \{\bot\}$ also to allow for deletion
(as discussed on the next page).
}
\end{quote}

We have corrected Definition~\ref{eq:delete} by allowing the range of $t$ and $t^*$ as $D_n \cup \{\bot\}$. 

The transformation of 
a database state $\mathcal{T}_c(D_n)$ excludes the complaint tuple $\{t\}$ (``$\setminus$'' is the set minus operation) 
and replace it with correct tuple $t^*$. More specifically, when $t = \bot$, it means tuple $t$ does not exist in the original
database and should be added into the database; similarly when $t^* = \bot$, the tuple $t$ is in
the original database while it should be removed. 

\begin{quote}
\reviewer{
p4. The paper often refers to ``data manipulation queries'', which sounds
self-contradictory to me (a query reads, an update writes). Perhaps just
``updates'' instead of ``data manipulation queries'' ?
}
\end{quote}

We have changed the term as ``data manipulation statements'' (includes \texttt{UPDATE},
\texttt{INSERT}, and \texttt{DELETE} queries) to avoid confusion. In fact, 
we intentionally avoid the term ``update queries'' in order to distinguish it from the 
specific \texttt{UPDATE} queries. 


\begin{quote}
\reviewer{
p5. How strongly does the performance of the MILP solver depend on M (the
upper bound chosen on the value of a given numeric field)? Presumably,
choosing the largest machine-representable number would lead to overflow
problems?
}
\end{quote}

We set $M$ to a value that is outside of the domain range in order to 
avoid generating conflict constraints. 
For example, attribute \textit{human age} has a domain range of $[0, 200]$, 
in which case $M$ must be set to a value that is above $200$. 
Besides, in order to maintain the feasibility for workloads with \texttt{DELETE}
queries, $M$ must be set to a value that is greater than or equal to $M^-$ (we have explained
the setting of $M^-$ in Section~\ref{sec:linearize}). 
In conclude, we typically set $M$ to a sufficiently large number which can be 
much smaller than the upper bound of a numeric field. Otherwise, we may
have overflow problems.

\begin{quote}
\reviewer{
p5. In (5), should v be u?
}
\end{quote}

We thank the reviewer for noting this, we have corrected this typo in the paper. 

\begin{quote}
\reviewer{
p7. ``Figure 3 showed the exponential cost'' - the scaling from 40 to 60 to 80 looked linear to me, just with a high coefficient.}
\end{quote}
We apologize for not explaining it clear. In Figure~\ref{fig:querysize_vs_time}, \naive fails to produce an answer
for problems with $\geq 80$ queries within 1000 sec as we terminate \naive once it reaches a time limit of 1000 sec. 
In fact, the execution cost of \naive shows exponential growth with $10$, $20$, $40$, and $60$ queries. 


\begin{quote}
\reviewer{
p10. Fig. 8 - please add 0.75 (the largest x-axis value) to the x-axis, better yet, label the x-axis with the rate values that correspond to data points.
}
\end{quote}

We thank the reviewer for the detailed suggestion and we have replaced Figure~\ref{f:falsenegative} with suggested x-axis labels. 

\begin{quote}
\reviewer{
p10. ``may suffer if the corruption occured in a very old query'' - does it (and are you saying fig. 8 supports this claim)?

p10. ``may over-generalize'' - again, are you speculating or interpreting the results here?
}
\end{quote}

We apologize for the imprecise wording and we have revised analysis for Figure~\ref{f:falsenegative}.

From Figure~\ref{f:falsenegative}, we find that reducing the number of reported complaints lowers the
runtime; however, we observe a small reduction in repair quality (precision
and recall in Figure~\ref{f:falsenegative_acc}) for recent corruptions and a
significant drop for older ones. This is expected: the less information we
have on the problem (fewer complaints), the lower our ability to fix it
correctly. In the extreme case where very few complaints are available, the
problem can be under-specified, and the true repair harder to identify.

\begin{quote}
\reviewer{
p10, footnote: this seems like a strong assumption! I guess this means that dealing with multiple errors that interact with each other is future work...
}
\end{quote}

We have clarified this assumption in Section~\ref{sec:abstractions}. Currently, 
we cannot apply incremental algorithm 
on problems with multiple corruptions since errors may scattered across the entire query history.
Thus, \sys does not scale over problems with large workload size and we leave further improvement in our future work.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

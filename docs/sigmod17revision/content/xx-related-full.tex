%!TEX root = ../main.tex

\section{Related Work}
\label{s:related} 
\sys tackles the problem of diagnosis and repair in relational query
histories (query logs). 
However, since \sys does not modify the query structure, one could
frame this as a data diagnosis and repair problem: Queries are represented in
a relation $R_q$ where each tuple is a record of a query and its parameters,
and the log is modeled as a single nested \texttt{SELECT} query over $R_q$ and
$D_0$. The result of this query is a view representing the final database
state $D_n$ and complaints are errors annotated on this view. Prior work has
focused on the problem of identifying the source tuples responsible for errors
in the output. However, the existing techniques are not effective for this
problem:
Some of the existing work focuses on aggregate queries~\cite{Wu13,
roy2014formal} and there is no direct mapping between an aggregate function
and individual error tuples. Descriptive and prescriptive
cleaning~\cite{chalamalla2014} uses the lineage of output errors to identify
the input tuples that are connected to most errors. This intuition is not
suitable for our problem setting, because there is no causal relationship
between the coverage of an update and the likelihood of error (a query that
updates the entire database is no more likely to be incorrect than a query
that updates a single tuple).
Finally, causality techniques~\cite{meliou2011tracing} are inefficient and
cannot scale because the problem size grows exponentially. Moreover, they do
not produce repairs.


\sys does not aim to correct errors in the data
directly, but rather to find the underlying reason for reported errors
in the queries that operated on the data. 
This is in contrast to
traditional data 
cleaning~\cite{dallachiesa2013nadeef, rahm00, Raman01, Kalashnikov06, Fan2008b
} which focuses on identifying and correcting data
``in-place.'' Identifying and correcting errors is an important, and
rightfully well-studied problem. Existing literature has supplied a
variety of tools to capture errors originating from data
integration~\cite{Abiteboul99
},
recognizing the same entities in data~\cite{Koudas2006, GruenheidDS14
}, identifying true facts among conflicting
data~\cite{yin2008truth, DN09, ltm2012
}, and language support for
cleaning~\cite{Galhardas2000}. All of these techniques are
complementary to our work. Their goal is to identify which data is
correct and which data is incorrect, but they don't look for the
sources of these errors in the processes or queries that generate and
modify this data. In fact the output of such methods can be used to
augment the complaint sets used by \sys, which focuses on identifying
errors in the queries that produced the data, rather than the data
itself.

An aspect of data cleaning focuses on providing repairs for the
identified errors~\cite{ChuIP13}. Tools in this domain have targeted
different methods and interactions for providing fixes, ranging from
rules~\cite{Beskales2010, Cong2007
} and functional
dependencies~\cite{Fan2008b, ChuIP13
}, to interactive methods that
solicit user feedback~\cite{Yakout, Raman01
}. As with the other data
cleaning approaches, all these techniques again operate on the data
directly. In contrast, \sys analyzes errors in the data to diagnose
and repair errors in queries that operated on the data. Thus, \sys
leverages the fact that some data errors are systemic, linked to
erroneous updates. Diagnosing the cause of the errors, will achieve
systematic fixes that will correct all relevant errors, even if they
have not been explicitly identified.

\looseness -1
Closer to exploring systemic reasons and patterns for errors are systems such as
 Data Auditor~\cite{GolabKKS10, Golab2008
} and Data X-Ray~\cite{wang2015}. Both tools tools identify features, which can
be selected from the tuple attributes, that best summarize or describe
groups of tuples (or specifically errors). While these tools can
generate feature sets or patterns of attributes that characterize
errors, these are not linked to the queries, but are again
characterizations over the data itself. Such techniques can be
tremendously useful if the processes that generate or modify the data
are unknown or black-box computations. In these cases, Data Auditor
and Data X-Ray can provide high-level clues for potential problems in
the data derivation process. However, both approaches are oblivious to
the actual queries that operated on the data, and they do not provide
particular fixes. Ontology-based why-not
explanations~\cite{tenCate2015} is similar to Data X-Ray, but 
only relevant to absent tuples (deletions), and does not
consider the query history.

The topic of query revisions has been studied in the context of
why-not explanations~\cite{Chapman2009}. These scenarios investigate
the absence of answers in a query result, and often attempt to modify
the query to change its outcome. Skyline
refinement~\cite{tran2010conquer} focuses specifically on refinements
that use skyline queries, while semi-automatic SQL
debugging~\cite{tzompanaki14semi} revises a given query to make it
return specified tuple groups in its output.  Furthermore, Query-by-example~\cite{zloof1977query} 
and query correction~\cite{abouzied2012dataplay} are similar problems that generate or modify
queries based on user interaction such as desired result records.
All these approaches are
limited to selection predicates of \texttt{SELECT} queries, and they
only typically consider one query at a time. In contrast, \sys handles
update workloads, processes large query histories, and can model
several steps in the dataset's evolution. A lot of explanation work~\cite{GebalyAGKS14, Khoussainova2012, Thirumuruganathan2012, Das2011, Fabbri2011, Bender14} targets specific application domains, limiting its applicability to our setting. 
% \xlw{Scorpion and Roy's explanation work conflict with newly added paraphraph on data source+select query. So I remove them here.  }

Finally, as \sys traces errors in the queries that manipulate data, it
has connections to the field of \emph{data and workflow provenance}.
Our algorithms build on several formalisms introduced by work in this
domain. These formalisms express why a particular data item appears in
a query result, or how that query result was produced in relation to
input data~\cite{BunemanKT01,GKT07-semirings, CheneyCT09, CuiWW00
}.
 


\section{Demonstration Outline}
\label{sec:demo}

In this section, we detail the proposed demonstration. The objective
of this demonstration is to show how \sys can quickly and accurately detect 
and propose fixes to errors in a query log, and compare its results to alternatives
that use existing techniques.


\begin{figure}[h]
\centering
  \includegraphics[width = .75\columnwidth]{figures/demo1}
  \caption{Users select and introduce errors to benchmark query workloads.}
  \label{f:demo1} 
\end{figure}

Figures~\ref{f:demo1} and~\ref{f:demo2} show screenshots of the initial and results pages.
Each step is annotated with a circled number, which we detail below.

\noindent {\bf Step 1 (Select Dataset):} Participants may first choose from a dropdown menu containing
a number of transaction workload generators from the benchmarks in OLTPBench~\cite{oltpbench}.
Since most transactional benchmarks focus on point update queries, we additionally include a 
synthetic workload generator that includes range updates, as well as insert and delete queries.
The text box on the right side allows users to additionally specify the number of queries to generate
in the workload.  

\noindent {\bf Step 2 (Corrupt the Query Log):} 
Once the workload generator is specified, the Query Log component of the interface
renders a scrollable list containing all of the queries.  When users select a query in the log,
the interface shows the query in an editable popup so that users can edit the queries, thus introducing errors
into the workload.  For example, in the figure, the user has edited query $Q_2$ by changing the \texttt{SET}
clause from $tax = 35$ to $tax = 45$.

\noindent {\bf Step 3:} The modified query cause the state of the database at the end of the workload
to differ from the result of the original workload.  The candidate complaints table lists the tuples
that are different and highlights the attribute values in those tuples as red text.  For instance,
$t_1.rate$ and $t_1.owed$ are both errors introduced by the modified query.  Users can select individual
attribute values or entire tuples to add to the complaint set that is used as input to the \sys algorithms.  
When she is satisfied, the user clicks \texttt{Run QueryFix} to execute the \sys and alternative algorithms


\begin{figure}[h]
\centering
  \includegraphics[width = .85\columnwidth]{figures/demo2}
  \caption{Comparisons between proposed fixes.}
  \label{f:demo2} 
\end{figure}

\noindent {\bf Step 4:}  The result page lists the original query ID and text at the top.  The ID 
is important because some proposed fixes may identify an incorrect query.  Below the original query,
the interface shows each of the proposed fixes as columns.  For example, Figure~\ref{f:demo2} shows 
that both the \sys and alternative fixes identified the correct query $Q_2$, however \sys only took $0.2$ seconds
to run, and correctly fixed $Q_2$, whereas the alternative took $5$ sec and proposed an incorrect fix.

\noindent {\bf Step 5:} The bottom tables show the effects of the fixes on the complaints from
step 3.  Correctly fixed attribute values are highlighted in blue, unfixed errors are shown as red text, while incorrectly
fixed values are highlighted with a red background.  Finally, it is possible for proposed fixes to 
{\it introduce} new errors, which are shown as entire rows that are highlighted with a red background.







